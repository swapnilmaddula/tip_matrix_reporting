{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17130e2b-2877-48d4-be86-79364e9841f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.functions import col, sha2, concat_ws\n",
    "\n",
    "\n",
    "class ReadAPI:\n",
    "    def __init__(self, api_url: str, folder_path_silver: str, primary_key: str):\n",
    "        \"\"\"\n",
    "        api_url: URL endpoint that returns JSON (a list of dicts)\n",
    "        folder_path_silver: Target location for silver data (e.g., DBFS or local path)\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.folder_path_silver = folder_path_silver\n",
    "        self.spark = SparkSession.builder.appName(\"TIP\").getOrCreate()\n",
    "        # self.spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "        self.username = dbutils.secrets.get(scope = \"tip_challenge\", key = \"tip_challenge_username\")\n",
    "        self.password = dbutils.secrets.get(scope = \"tip_challenge\", key = \"tip_challenge_password\")\n",
    "        \n",
    "        self.primary_key = primary_key\n",
    "\n",
    "        if not self.username or not self.password:\n",
    "            raise ValueError(\"Missing environment variables: tip_challenge_username or tip_challenge_password\")\n",
    "\n",
    "    def fetch_api_data(self):\n",
    "        try:\n",
    "            response = requests.get(self.api_url, auth=(self.username, self.password))\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            df = self.spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "            return df \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching API data: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def incremental_load(self):\n",
    "\n",
    "        new_data = self.fetch_api_data()\n",
    "\n",
    "        #add primary_key, this can also be imported from the config, and then used as part of the metadata ingestion framework\n",
    "        if self.primary_key != '':\n",
    "            new_data = new_data.withColumn(\"primary_key\", col(self.primary_key))\n",
    "        else:\n",
    "            print(\"primary key not found, hashing on all columns\")\n",
    "            new_data = new_data.withColumn(\n",
    "                \"primary_key\",\n",
    "                sha2(concat_ws(\"||\", *[col(c).cast(\"string\") for c in new_data.columns]), 256)\n",
    "            )\n",
    "\n",
    "        new_data.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.folder_path_silver}\n",
    "        USING DELTA\n",
    "        AS SELECT * FROM new_data WHERE 1 = 0\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {self.folder_path_silver} AS t\n",
    "            USING new_data AS s\n",
    "            ON t.primary_key = s.primary_key\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcbd4387-e32a-477d-b083-44c17146772a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading table silver.wot_log\nprimary key not found, hashing on all columns\n"
     ]
    }
   ],
   "source": [
    "#metadata ingestion framework, configs can be passed through a database for example or config files\n",
    "\n",
    "jobs = [\n",
    "    {\n",
    "        \"api\": \"http://hiringchallenge.efdwfudnhvaebpej.northeurope.azurecontainer.io/WorkShopManagement\",\n",
    "        \"folder_path_silver\": \"silver.workshop_management\",\n",
    "        \"primary_key\": \"WSM_Key\"\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"api\": \"http://hiringchallenge.efdwfudnhvaebpej.northeurope.azurecontainer.io/WOTLog\",\n",
    "        \"folder_path_silver\": \"silver.wot_log\",\n",
    "        \"primary_key\": \"\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "for j in jobs:\n",
    "\n",
    "    print(f\"loading table {j[\"folder_path_silver\"]}\")\n",
    "\n",
    "    read_api = ReadAPI(\n",
    "        api_url=j[\"api\"],\n",
    "        folder_path_silver=j[\"folder_path_silver\"],\n",
    "        primary_key = j[\"primary_key\"]\n",
    "    )\n",
    "    read_api.fetch_api_data()\n",
    "    try:\n",
    "        read_api.incremental_load()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6390519833786891,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_from_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}